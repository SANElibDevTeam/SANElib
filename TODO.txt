TO DO List

Admin
- [x] Meeting
- [ ] A way to organize ourselves: who works on what issues or text file?
=> every  task on the code, open an issue
- [ ] Anna opens issue (Anna)
- [ ] get Github runninng (MK) <= upload / merge new code with Gabriels changes

Current
- [x] 1 feature  (+ bin size if num) training acc rechnen
    - [x] 1d => simple, fast look up
- [x] All features 1d accuracy  training accuracy (MK)
- [x] Generate mdim SQL (MK)
- rank all features (information? R Skript? mutinf?)
- uplodad github

- [] All features 1d bin size (MK) just take 50

-- [?] default predictor into materialized view
-- code aufräumen, nur was echt gebraucht wird rechnen, z.B. pmi etc weg
-- y weg, nur noch multidim joint prob rechnen für _m; n_y wird eh nicht gebraucht, nxy besser
- [ ] covtyp all features hyperpareter , note how to do this (MK)
- [ ] Compare densities / probabilities (MK) => interesting for the paper, try log-likielihoods, scikit-learn returns these values
- [] all templates using Jinja 2

- [x] GitHub (Anna)
- [...] Steam DB (Anna)
- [ ] State of the art (Anna, Gabe, MK) future, new databases for ML such as MLDB, ...
- [ ] Shared folder on ONeDRive <= for the Paper
- [ ] SQLalchemy (anna)
- [x] benchmark covtyp => annas new code (one hot is better for random foest and other)

Publication deadline: 5th of February
- [] First Draft Mid January
- [ ] Paper Data Science conf.
- [ ] Steam Benchmark sql
- [ ] steam benchmark sklearn
- [ ] Describe the Method (MK) => Estimate P(X1,X2, ..., Xn, Y)
- [ ] Publication budget with Eckart (MK)

More Ideas (Nice to Have)
- [ ] Visualization, 1D, 2D (Gabe)
- [ ] Smoothing
- [ ] Interpolation
- [ ] Naive Bayes multiplcation
- [ ] Automatic Hyperparameter opt
- [ ] Get optimal n bins (training acc.) Start with 15 +/- 5, if optimal try steps +/- 1; avg +/ stddev
- [ ] Load data to db
- [ ] Split test train
- [ ] Copy & index source table for speed
- [ ] Default embedded DB (SQLite?
- [.. ] Default Parameters (Gabe)
- [ ] One line start (with skleran toy data)

More SQL Analytics possibilities
- [ ] Idea: Random Restaurant / database (High-Dimensional)
- [ ] Clustering
- [ ] Regression
- [ ] Empirical estimation : n buckets default
- [ ] Categorical split


## Other useful functions / methods for data scientists:
## * Get list of features ranked by 1D prediction accuracy
## * Optimize n Buckets for 1 feature
## * Visualize 1D and 2D Decision Table
## * Load Data into a DB Table
## * Split Training / Test
## * Remove Model from DB (drop all tables for model_id)


# **I plan on making this more functional**: Make it so not as many functions need to be called

# Initialization with the required variables
# As stated in the documentation above, I think we may want to have the user pass in the target and feature
# variables similiar to how it is done in skikit - I.e: Numpy arrays, etc. But not sure how SQL would like this
# MK --> perhaps SQLalchemy can be of help here.

* y <-> x density estimation, target for prediction

Idee: SQL Server Pages
Statt in Pyton SQL generieren => in SQL Python annotieren